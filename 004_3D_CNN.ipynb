{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base 3D CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow tensorflow-metal scikit-learn ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size_pixels = 64\n",
    "model_input_width = 6  # number of images before the event\n",
    "buffer_size = 100\n",
    "# num_classes = 6\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "datasets_folder = os.path.join(\"E:\\\\Workspace\\\\Thesis_dataset\", \"illinois_dataset\")\n",
    "# dataset_folder_name = \"Quercus_germany_200_stations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Workspace\\Thesis_dataset\\illinois_dataset\\train_data.npz\n",
      "E:\\Workspace\\Thesis_dataset\\illinois_dataset\\train.csv\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_folder_path = datasets_folder\n",
    "os.makedirs(dataset_folder_path, exist_ok=True)\n",
    "\n",
    "record_id_column_name = 'record_id'\n",
    "label_column_name = \"ordered_phase_id\"\n",
    "\n",
    "train_timeseries_file_path = os.path.join(dataset_folder_path, \"train_data.npz\")\n",
    "print(train_timeseries_file_path)\n",
    "train_label_file_path = os.path.join(dataset_folder_path, \"train.csv\")\n",
    "print(train_label_file_path)\n",
    "print(\"-\" * 60)\n",
    "# val_timeseries_file_path = os.path.join(dataset_folder_path, \"val_data.npz\")\n",
    "# print(val_timeseries_file_path)\n",
    "# val_label_file_path = os.path.join(dataset_folder_path, \"val.csv\")\n",
    "# print(val_label_file_path)\n",
    "# print(\"-\" * 60)\n",
    "# test_timeseries_file_path = os.path.join(dataset_folder_path, \"test_data.npz\")\n",
    "# print(test_timeseries_file_path)\n",
    "# test_label_file_path = os.path.join(dataset_folder_path, \"test.csv\")\n",
    "# print(test_label_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64, 64, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_file = np.load(train_timeseries_file_path, mmap_mode=\"r\")\n",
    "first_array_key = next(iter(npz_file))\n",
    "timeseries_shape = npz_file[first_array_key].shape\n",
    "timeseries_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv_path = r\"E:\\Workspace\\Thesis_dataset\\illinois_dataset\\train.csv\"\n",
    "test_label_file_path = r\"E:\\Workspace\\Thesis_dataset\\illinois_dataset\\test.csv\"\n",
    "record_id_column_name = \"id\"\n",
    "label_column_name = \"label\"\n",
    "timeseries_npz_path = r\"E:\\Workspace\\Thesis_dataset\\illinois_dataset\\train_data.npz\"\n",
    "test_timeseries_file_path = r\"E:\\Workspace\\Thesis_dataset\\illinois_dataset\\train_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    label_csv_path, record_id_column_name, label_column_name, timeseries_npz_path\n",
    "):\n",
    "\n",
    "    # Check if the inputs are a byte string and decode it to a regular string if necessary\n",
    "    if isinstance(label_csv_path, bytes):\n",
    "        label_csv_path = label_csv_path.decode(\"utf-8\")\n",
    "    if isinstance(timeseries_npz_path, bytes):\n",
    "        timeseries_npz_path = timeseries_npz_path.decode(\"utf-8\")\n",
    "    if isinstance(record_id_column_name, bytes):\n",
    "        record_id_column_name = record_id_column_name.decode(\"utf-8\")\n",
    "    if isinstance(label_column_name, bytes):\n",
    "        label_column_name = label_column_name.decode(\"utf-8\")\n",
    "\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(label_csv_path)\n",
    "\n",
    "    # Load the npz file\n",
    "    with np.load(timeseries_npz_path, allow_pickle=True) as npz_file:\n",
    "        for _, row in labels_df.iterrows():\n",
    "            record_id = str(\n",
    "                row[record_id_column_name]\n",
    "            )  # Ensure record_id is treated as a string\n",
    "            target = row[label_column_name]\n",
    "            if record_id in npz_file.files:\n",
    "                # Extract the time series data using record_id\n",
    "                time_series = npz_file[record_id]\n",
    "                # print(record_id, target)\n",
    "                yield time_series, target\n",
    "\n",
    "\n",
    "# Determine the output types\n",
    "output_types = (tf.float32, tf.int64)\n",
    "\n",
    "# Determine the output shapes\n",
    "output_shapes = (timeseries_shape, ())  #  ((6, 11, 64, 64), ())\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,  # Generator function\n",
    "    args=(\n",
    "        train_label_file_path,\n",
    "        record_id_column_name,\n",
    "        label_column_name,\n",
    "        train_timeseries_file_path,\n",
    "    ),  # Arguments to pass to the generator\n",
    "    output_types=output_types,\n",
    "    output_shapes=output_shapes,\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.shuffle(buffer_size=buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# # Create validation dataset\n",
    "# val_dataset = tf.data.Dataset.from_generator(\n",
    "#     data_generator,  # Generator function\n",
    "#     args=(\n",
    "#         val_label_file_path,\n",
    "#         record_id_column_name,\n",
    "#         label_column_name,\n",
    "#         val_timeseries_file_path,\n",
    "#     ),  # Arguments to pass to the generator\n",
    "#     output_types=output_types,\n",
    "#     output_shapes=output_shapes,\n",
    "# )\n",
    "# val_dataset = (\n",
    "#     val_dataset.shuffle(buffer_size=buffer_size)\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(tf.data.AUTOTUNE)\n",
    "# )\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,  # Generator function\n",
    "    args=(\n",
    "        test_label_file_path,\n",
    "        record_id_column_name,\n",
    "        label_column_name,\n",
    "        test_timeseries_file_path,\n",
    "    ),  # Arguments to pass to the generator\n",
    "    output_types=output_types,\n",
    "    output_shapes=output_shapes,\n",
    ")\n",
    "test_dataset = test_dataset.shuffle(buffer_size=buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} KeyError: 'label'\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3790, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n\nKeyError: 'label'\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Surya Naganathan\\AppData\\Local\\Temp\\ipykernel_31988\\226997384.py\", line 24, in data_generator\n    target = row[label_column_name]\n             ~~~^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py\", line 1040, in __getitem__\n    return self._get_value(key)\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py\", line 1156, in _get_value\n    loc = self.index.get_loc(label)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3797, in get_loc\n    raise KeyError(key) from err\n\nKeyError: 'label'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Print the first batch of data\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3113\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3111\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3113\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3115\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} KeyError: 'label'\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3790, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n\nKeyError: 'label'\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Surya Naganathan\\AppData\\Local\\Temp\\ipykernel_31988\\226997384.py\", line 24, in data_generator\n    target = row[label_column_name]\n             ~~~^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py\", line 1040, in __getitem__\n    return self._get_value(key)\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py\", line 1156, in _get_value\n    loc = self.index.get_loc(label)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3797, in get_loc\n    raise KeyError(key) from err\n\nKeyError: 'label'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "for data in train_dataset.take(1):  # Print the first batch of data\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train Dataset ---\n",
      "Shape of data: (8, 6, 64, 64, 15)\n",
      "Shape of label: (8,)\n",
      "--- Test Dataset ---\n",
      "Shape of data: (8, 6, 64, 64, 15)\n",
      "Shape of label: (8,)\n"
     ]
    }
   ],
   "source": [
    "# Function to print the shapes of the dataset\n",
    "def print_dataset_shapes(dataset, dataset_name=\"Dataset\"):\n",
    "    print(f\"--- {dataset_name} ---\")\n",
    "    for data, label in dataset.take(1):  # Take a single batch from the dataset\n",
    "        print(f\"Shape of data: {data.shape}\")\n",
    "        print(f\"Shape of label: {label.shape}\")\n",
    "\n",
    "# Print shapes for train, validation, and test datasets\n",
    "print_dataset_shapes(train_dataset, \"Train Dataset\")\n",
    "# print_dataset_shapes(val_dataset, \"Validation Dataset\")\n",
    "print_dataset_shapes(test_dataset, \"Test Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalization_stats(dataset):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of the dataset.\n",
    "    \n",
    "    Args:\n",
    "    dataset: A TensorFlow Dataset from which the statistics should be computed.\n",
    "\n",
    "    Returns:\n",
    "    mean: Mean of the dataset.\n",
    "    stddev: Standard deviation of the dataset.\n",
    "    \"\"\"\n",
    "    # Initialize variables to compute mean and variance\n",
    "    total_sum = 0\n",
    "    total_squared_sum = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Iterate through the dataset to compute the sum and squared sum\n",
    "    for data, _ in dataset:\n",
    "        total_sum += tf.reduce_sum(data, axis=0)\n",
    "        total_squared_sum += tf.reduce_sum(tf.square(data), axis=0)\n",
    "        total_count += tf.cast(tf.shape(data)[0], tf.float32)\n",
    "\n",
    "    # Compute the mean and variance\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - tf.square(mean)\n",
    "    stddev = tf.sqrt(variance)\n",
    "    \n",
    "    return mean, stddev\n",
    "\n",
    "def normalize_dataset(dataset, mean, stddev):\n",
    "    \"\"\"\n",
    "    Normalize the dataset using mean and standard deviation, replacing NaN values with the mean.\n",
    "\n",
    "    Args:\n",
    "    dataset: A TensorFlow Dataset that needs to be normalized.\n",
    "    mean: The mean values for each feature.\n",
    "    stddev: The standard deviation for each feature.\n",
    "\n",
    "    Returns:\n",
    "    A normalized TensorFlow Dataset.\n",
    "    \"\"\"\n",
    "    # Function to replace NaN values with the mean and normalize\n",
    "    def replace_nan_and_normalize(data, target):\n",
    "        # Replace NaN values with the mean\n",
    "        data_no_nan = tf.where(tf.math.is_nan(data), mean, data)\n",
    "        # Apply normalization using broadcasting\n",
    "        normalized_data = (data_no_nan - mean) / stddev\n",
    "        return normalized_data, target\n",
    "\n",
    "    # Apply the replace and normalization function\n",
    "    return dataset.map(replace_nan_and_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m normalize_dataset(train_dataset, mean, stddev)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Normalize the validation dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m normalize_dataset(\u001b[43mval_dataset\u001b[49m, mean, stddev)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Normalize the test dataset\u001b[39;00m\n\u001b[0;32m     11\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m normalize_dataset(test_dataset, mean, stddev)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset is a tf.data.Dataset object\n",
    "mean, stddev = compute_normalization_stats(train_dataset)\n",
    "\n",
    "# Normalize the train dataset\n",
    "train_dataset = normalize_dataset(train_dataset, mean, stddev)\n",
    "\n",
    "# Normalize the validation dataset\n",
    "val_dataset = normalize_dataset(val_dataset, mean, stddev)\n",
    "\n",
    "# Normalize the test dataset\n",
    "test_dataset = normalize_dataset(test_dataset, mean, stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First Record from Train Dataset ---\n",
      "Data: [[[[ 1.00000000e+00  2.19300000e+03  2.14400000e+03 ... -2.82243614e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.19100000e+03  2.16900000e+03 ... -2.67470703e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.19200000e+03  2.19300000e+03 ... -2.46584034e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  1.84400000e+03  1.84100000e+03 ... -2.39988976e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.82300000e+03  1.82200000e+03 ... -2.74496632e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.80300000e+03  1.80400000e+03 ... -3.11657639e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00  2.20500000e+03  2.16600000e+03 ... -2.65925369e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.20300000e+03  2.17900000e+03 ... -2.61412239e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.20100000e+03  2.19200000e+03 ... -2.55246410e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  1.84800000e+03  1.83700000e+03 ... -2.38298798e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.83100000e+03  1.82500000e+03 ... -2.67509727e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.81500000e+03  1.81400000e+03 ... -2.99931545e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00  2.22600000e+03  2.20400000e+03 ... -2.38064957e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.22300000e+03  2.19600000e+03 ... -2.51068516e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.21800000e+03  2.19000000e+03 ... -2.70035858e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  1.85500000e+03  1.83000000e+03 ... -2.35413132e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.84400000e+03  1.83100000e+03 ... -2.55580864e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.83700000e+03  1.83200000e+03 ... -2.79911385e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  2.55700000e+03  2.54100000e+03 ... -1.90535564e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.55500000e+03  2.55100000e+03 ... -2.00430813e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.54600000e+03  2.55600000e+03 ... -2.10738983e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  2.30700000e+03  2.26000000e+03 ... -2.14453392e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.27600000e+03  2.23500000e+03 ... -2.09138985e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  2.24300000e+03  2.21100000e+03 ... -2.08924541e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[ 1.00000000e+00  3.05900000e+03  2.90900000e+03 ... -2.82243614e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  3.36000000e+03  3.20200000e+03 ... -2.67470703e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  3.77400000e+03  3.56100000e+03 ... -2.46584034e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  1.07600000e+03  1.14100000e+03 ... -2.39988976e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.30800000e+03  1.11600000e+03 ... -2.74496632e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.54500000e+03  1.06600000e+03 ... -3.11657639e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00  3.44900000e+03  3.21000000e+03 ... -2.65925369e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  3.79300000e+03  3.54700000e+03 ... -2.61412239e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  4.23700000e+03  3.94800000e+03 ... -2.55246410e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  9.34000000e+02  1.12100000e+03 ... -2.38298798e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.05000000e+03  1.05700000e+03 ... -2.67509727e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.19500000e+03  9.69000000e+02 ... -2.99931545e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00  4.11400000e+03  3.72300000e+03 ... -2.38064957e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  4.53300000e+03  4.13800000e+03 ... -2.51068516e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  5.02800000e+03  4.60700000e+03 ... -2.70035858e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  6.90000000e+02  1.08600000e+03 ... -2.35413132e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  6.10000000e+02  9.57000000e+02 ... -2.55580864e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  5.98000000e+02  8.03000000e+02 ... -2.79911385e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  9.82000000e+02  1.18500000e+03 ... -1.90535564e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.26400000e+03  1.32300000e+03 ... -2.00430813e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  1.52400000e+03  1.43400000e+03 ... -2.10738983e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.36700000e+03  2.61200000e+03 ... -2.14453392e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  3.29300000e+03  2.39500000e+03 ... -2.09138985e+01\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00  3.24400000e+03  2.18000000e+03 ... -2.08924541e+01\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     0.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[ 1.00000000e+00  3.35800000e+03  3.52300000e+03 ... -2.82243614e+01\n",
      "     3.42794609e+01  2.44286518e+01]\n",
      "   [ 1.00000000e+00  2.87000000e+03  2.98300000e+03 ... -2.67470703e+01\n",
      "     3.42843094e+01  2.44289455e+01]\n",
      "   [ 1.00000000e+00  2.32500000e+03  2.36700000e+03 ... -2.46584034e+01\n",
      "     3.42907257e+01  2.44293346e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.44000000e+02  5.47000000e+02 ... -2.39988976e+01\n",
      "     3.46445580e+01  2.44534760e+01]\n",
      "   [ 1.00000000e+00  3.22000000e+02  5.36000000e+02 ... -2.74496632e+01\n",
      "     3.46350021e+01  2.44546242e+01]\n",
      "   [ 1.00000000e+00  3.19000000e+02  5.43000000e+02 ... -3.11657639e+01\n",
      "     3.46277809e+01  2.44554920e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  2.97900000e+03  3.07100000e+03 ... -2.65925369e+01\n",
      "     3.42990265e+01  2.44319077e+01]\n",
      "   [ 1.00000000e+00  2.64800000e+03  2.72700000e+03 ... -2.61412239e+01\n",
      "     3.43070641e+01  2.44321098e+01]\n",
      "   [ 1.00000000e+00  2.28500000e+03  2.34300000e+03 ... -2.55246410e+01\n",
      "     3.43164787e+01  2.44324150e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.42000000e+02  5.46000000e+02 ... -2.38298798e+01\n",
      "     3.46579781e+01  2.44572029e+01]\n",
      "   [ 1.00000000e+00  3.26000000e+02  5.40000000e+02 ... -2.67509727e+01\n",
      "     3.46497917e+01  2.44582825e+01]\n",
      "   [ 1.00000000e+00  3.23000000e+02  5.46000000e+02 ... -2.99931545e+01\n",
      "     3.46426659e+01  2.44591408e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  2.33100000e+03  2.29900000e+03 ... -2.38064957e+01\n",
      "     3.43324318e+01  2.44374657e+01]\n",
      "   [ 1.00000000e+00  2.27000000e+03  2.28900000e+03 ... -2.51068516e+01\n",
      "     3.43459129e+01  2.44375153e+01]\n",
      "   [ 1.00000000e+00  2.21600000e+03  2.30000000e+03 ... -2.70035858e+01\n",
      "     3.43604469e+01  2.44376774e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.39000000e+02  5.45000000e+02 ... -2.35413132e+01\n",
      "     3.46808929e+01  2.44635677e+01]\n",
      "   [ 1.00000000e+00  3.33000000e+02  5.46000000e+02 ... -2.55580864e+01\n",
      "     3.46750450e+01  2.44645271e+01]\n",
      "   [ 1.00000000e+00  3.31000000e+02  5.52000000e+02 ... -2.79911385e+01\n",
      "     3.46680832e+01  2.44653702e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  8.52000000e+02  1.14100000e+03 ... -1.90535564e+01\n",
      "     3.45924797e+01  2.47602787e+01]\n",
      "   [ 1.00000000e+00  8.48000000e+02  1.15800000e+03 ... -2.00430813e+01\n",
      "     3.46066818e+01  2.47605305e+01]\n",
      "   [ 1.00000000e+00  8.25000000e+02  1.16100000e+03 ... -2.10738983e+01\n",
      "     3.46254692e+01  2.47608662e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  9.82000000e+02  1.33300000e+03 ... -2.14453392e+01\n",
      "     3.55608711e+01  2.47806149e+01]\n",
      "   [ 1.00000000e+00  8.62000000e+02  1.19500000e+03 ... -2.09138985e+01\n",
      "     3.55766335e+01  2.47809486e+01]\n",
      "   [ 1.00000000e+00  7.42000000e+02  1.05600000e+03 ... -2.08924541e+01\n",
      "     3.55885468e+01  2.47812023e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.45924797e+01  2.47602787e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.46066818e+01  2.47605305e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.46254692e+01  2.47608662e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55608711e+01  2.47806149e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55766335e+01  2.47809486e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55885468e+01  2.47812023e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.45924797e+01  2.47602787e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.46066818e+01  2.47605305e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.46254692e+01  2.47608662e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55608711e+01  2.47806149e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55766335e+01  2.47809486e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.55885468e+01  2.47812023e+01]]]\n",
      "\n",
      "\n",
      " [[[ 1.00000000e+00  4.12000000e+02  5.80000000e+02 ... -2.82243614e+01\n",
      "     2.66723423e+01  1.56411219e+01]\n",
      "   [ 1.00000000e+00  4.17000000e+02  5.74000000e+02 ... -2.67470703e+01\n",
      "     2.66802425e+01  1.56423597e+01]\n",
      "   [ 1.00000000e+00  4.23000000e+02  5.75000000e+02 ... -2.46584034e+01\n",
      "     2.66906967e+01  1.56439972e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.14000000e+02  4.81000000e+02 ... -2.39988976e+01\n",
      "     2.73036480e+01  1.57387590e+01]\n",
      "   [ 1.00000000e+00  3.02000000e+02  4.73000000e+02 ... -2.74496632e+01\n",
      "     2.73116493e+01  1.57391939e+01]\n",
      "   [ 1.00000000e+00  2.96000000e+02  4.76000000e+02 ... -3.11657639e+01\n",
      "     2.73176975e+01  1.57395220e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  4.19000000e+02  6.00000000e+02 ... -2.65925369e+01\n",
      "     2.66821442e+01  1.56381168e+01]\n",
      "   [ 1.00000000e+00  4.20000000e+02  5.90000000e+02 ... -2.61412239e+01\n",
      "     2.66922054e+01  1.56387930e+01]\n",
      "   [ 1.00000000e+00  4.22000000e+02  5.84000000e+02 ... -2.55246410e+01\n",
      "     2.67047081e+01  1.56398945e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.11000000e+02  4.81000000e+02 ... -2.38298798e+01\n",
      "     2.73090591e+01  1.57370930e+01]\n",
      "   [ 1.00000000e+00  3.02000000e+02  4.76000000e+02 ... -2.67509727e+01\n",
      "     2.73171215e+01  1.57376680e+01]\n",
      "   [ 1.00000000e+00  2.98000000e+02  4.77000000e+02 ... -2.99931545e+01\n",
      "     2.73230610e+01  1.57380257e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  4.31000000e+02  6.35000000e+02 ... -2.38064957e+01\n",
      "     2.66988792e+01  1.56329870e+01]\n",
      "   [ 1.00000000e+00  4.26000000e+02  6.18000000e+02 ... -2.51068516e+01\n",
      "     2.67126312e+01  1.56327028e+01]\n",
      "   [ 1.00000000e+00  4.21000000e+02  6.01000000e+02 ... -2.70035858e+01\n",
      "     2.67286339e+01  1.56328878e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  3.05000000e+02  4.81000000e+02 ... -2.35413132e+01\n",
      "     2.73182983e+01  1.57342491e+01]\n",
      "   [ 1.00000000e+00  3.03000000e+02  4.80000000e+02 ... -2.55580864e+01\n",
      "     2.73264637e+01  1.57350636e+01]\n",
      "   [ 1.00000000e+00  3.02000000e+02  4.79000000e+02 ... -2.79911385e+01\n",
      "     2.73322201e+01  1.57354717e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  6.38000000e+02  8.66000000e+02 ... -1.90535564e+01\n",
      "     2.72502174e+01  1.55964756e+01]\n",
      "   [ 1.00000000e+00  6.47000000e+02  8.90000000e+02 ... -2.00430813e+01\n",
      "     2.72639790e+01  1.55952253e+01]\n",
      "   [ 1.00000000e+00  6.37000000e+02  8.90000000e+02 ... -2.10738983e+01\n",
      "     2.72821865e+01  1.55935717e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  7.15000000e+02  9.85000000e+02 ... -2.14453392e+01\n",
      "     2.79556370e+01  1.57265816e+01]\n",
      "   [ 1.00000000e+00  6.27000000e+02  8.77000000e+02 ... -2.09138985e+01\n",
      "     2.79668541e+01  1.57289448e+01]\n",
      "   [ 1.00000000e+00  5.40000000e+02  7.72000000e+02 ... -2.08924541e+01\n",
      "     2.79753342e+01  1.57307301e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72502174e+01  1.55964756e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72639790e+01  1.55952253e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72821865e+01  1.55935717e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79556370e+01  1.57265816e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79668541e+01  1.57289448e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79753342e+01  1.57307301e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72502174e+01  1.55964756e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72639790e+01  1.55952253e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.72821865e+01  1.55935717e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79556370e+01  1.57265816e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79668541e+01  1.57289448e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.79753342e+01  1.57307301e+01]]]\n",
      "\n",
      "\n",
      " [[[ 1.00000000e+00  4.16000000e+02  5.92000000e+02 ... -2.82243614e+01\n",
      "     3.12264080e+01  1.72594280e+01]\n",
      "   [ 1.00000000e+00  4.10000000e+02  5.94000000e+02 ... -2.67470703e+01\n",
      "     3.12450466e+01  1.72597141e+01]\n",
      "   [ 1.00000000e+00  4.13000000e+02  6.01000000e+02 ... -2.46584034e+01\n",
      "     3.12697067e+01  1.72600937e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  2.46000000e+02  4.36000000e+02 ... -2.39988976e+01\n",
      "     3.26280060e+01  1.72837620e+01]\n",
      "   [ 1.00000000e+00  2.42000000e+02  4.31000000e+02 ... -2.74496632e+01\n",
      "     3.25902252e+01  1.72849560e+01]\n",
      "   [ 1.00000000e+00  2.46000000e+02  4.33000000e+02 ... -3.11657639e+01\n",
      "     3.25616722e+01  1.72858582e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  4.07000000e+02  5.95000000e+02 ... -2.65925369e+01\n",
      "     3.12500038e+01  1.72608814e+01]\n",
      "   [ 1.00000000e+00  3.97000000e+02  5.91000000e+02 ... -2.61412239e+01\n",
      "     3.12674160e+01  1.72611523e+01]\n",
      "   [ 1.00000000e+00  3.95000000e+02  5.92000000e+02 ... -2.55246410e+01\n",
      "     3.12906513e+01  1.72615147e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  2.47000000e+02  4.34000000e+02 ... -2.38298798e+01\n",
      "     3.26277466e+01  1.72843685e+01]\n",
      "   [ 1.00000000e+00  2.44000000e+02  4.30000000e+02 ... -2.67509727e+01\n",
      "     3.25962296e+01  1.72854748e+01]\n",
      "   [ 1.00000000e+00  2.45000000e+02  4.32000000e+02 ... -2.99931545e+01\n",
      "     3.25688629e+01  1.72863617e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  3.92000000e+02  6.00000000e+02 ... -2.38064957e+01\n",
      "     3.12902908e+01  1.72633629e+01]\n",
      "   [ 1.00000000e+00  3.76000000e+02  5.87000000e+02 ... -2.51068516e+01\n",
      "     3.13056087e+01  1.72636070e+01]\n",
      "   [ 1.00000000e+00  3.65000000e+02  5.75000000e+02 ... -2.70035858e+01\n",
      "     3.13264103e+01  1.72639408e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  2.49000000e+02  4.31000000e+02 ... -2.35413132e+01\n",
      "     3.26273117e+01  1.72854080e+01]\n",
      "   [ 1.00000000e+00  2.46000000e+02  4.30000000e+02 ... -2.55580864e+01\n",
      "     3.26064835e+01  1.72863636e+01]\n",
      "   [ 1.00000000e+00  2.43000000e+02  4.30000000e+02 ... -2.79911385e+01\n",
      "     3.25811386e+01  1.72872219e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  7.12000000e+02  9.65000000e+02 ... -1.90535564e+01\n",
      "     3.16446419e+01  1.73464088e+01]\n",
      "   [ 1.00000000e+00  6.56000000e+02  9.18000000e+02 ... -2.00430813e+01\n",
      "     3.16602364e+01  1.73472939e+01]\n",
      "   [ 1.00000000e+00  5.89000000e+02  8.57000000e+02 ... -2.10738983e+01\n",
      "     3.16808662e+01  1.73484669e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  7.09000000e+02  9.83000000e+02 ... -2.14453392e+01\n",
      "     3.27448120e+01  1.73790798e+01]\n",
      "   [ 1.00000000e+00  6.10000000e+02  8.65000000e+02 ... -2.09138985e+01\n",
      "     3.27627678e+01  1.73795795e+01]\n",
      "   [ 1.00000000e+00  5.19000000e+02  7.56000000e+02 ... -2.08924541e+01\n",
      "     3.27763367e+01  1.73799591e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16446419e+01  1.73464088e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16602364e+01  1.73472939e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16808662e+01  1.73484669e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27448120e+01  1.73790798e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27627678e+01  1.73795795e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27763367e+01  1.73799591e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16446419e+01  1.73464088e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16602364e+01  1.73472939e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.16808662e+01  1.73484669e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27448120e+01  1.73790798e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27627678e+01  1.73795795e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     3.27763367e+01  1.73799591e+01]]]\n",
      "\n",
      "\n",
      " [[[ 1.00000000e+00  6.43000000e+02  7.75000000e+02 ... -2.82243614e+01\n",
      "     2.89995823e+01  2.09163761e+01]\n",
      "   [ 1.00000000e+00  6.43000000e+02  7.72000000e+02 ... -2.67470703e+01\n",
      "     2.89969845e+01  2.09169502e+01]\n",
      "   [ 1.00000000e+00  6.48000000e+02  7.75000000e+02 ... -2.46584034e+01\n",
      "     2.89935474e+01  2.09177094e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  5.51000000e+02  7.05000000e+02 ... -2.39988976e+01\n",
      "     2.87970810e+01  2.09606209e+01]\n",
      "   [ 1.00000000e+00  5.48000000e+02  7.05000000e+02 ... -2.74496632e+01\n",
      "     2.87977219e+01  2.09601498e+01]\n",
      "   [ 1.00000000e+00  5.48000000e+02  7.09000000e+02 ... -3.11657639e+01\n",
      "     2.87982063e+01  2.09597950e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  6.47000000e+02  7.75000000e+02 ... -2.65925369e+01\n",
      "     2.90089741e+01  2.09146881e+01]\n",
      "   [ 1.00000000e+00  6.45000000e+02  7.68000000e+02 ... -2.61412239e+01\n",
      "     2.90064697e+01  2.09161968e+01]\n",
      "   [ 1.00000000e+00  6.46000000e+02  7.65000000e+02 ... -2.55246410e+01\n",
      "     2.90031261e+01  2.09178581e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  5.46000000e+02  7.02000000e+02 ... -2.38298798e+01\n",
      "     2.88054562e+01  2.09605598e+01]\n",
      "   [ 1.00000000e+00  5.45000000e+02  7.02000000e+02 ... -2.67509727e+01\n",
      "     2.88056335e+01  2.09602261e+01]\n",
      "   [ 1.00000000e+00  5.45000000e+02  7.05000000e+02 ... -2.99931545e+01\n",
      "     2.88059978e+01  2.09599056e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00  6.54000000e+02  7.74000000e+02 ... -2.38064957e+01\n",
      "     2.90250072e+01  2.09118080e+01]\n",
      "   [ 1.00000000e+00  6.49000000e+02  7.60000000e+02 ... -2.51068516e+01\n",
      "     2.90226612e+01  2.09149113e+01]\n",
      "   [ 1.00000000e+00  6.43000000e+02  7.49000000e+02 ... -2.70035858e+01\n",
      "     2.90194798e+01  2.09181118e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  5.38000000e+02  6.97000000e+02 ... -2.35413132e+01\n",
      "     2.88197536e+01  2.09604588e+01]\n",
      "   [ 1.00000000e+00  5.39000000e+02  6.97000000e+02 ... -2.55580864e+01\n",
      "     2.88191433e+01  2.09603558e+01]\n",
      "   [ 1.00000000e+00  5.40000000e+02  6.99000000e+02 ... -2.79911385e+01\n",
      "     2.88193016e+01  2.09600945e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00000000e+00  7.76000000e+02  9.46000000e+02 ... -1.90535564e+01\n",
      "     2.92186794e+01  2.08743992e+01]\n",
      "   [ 1.00000000e+00  7.26000000e+02  8.79000000e+02 ... -2.00430813e+01\n",
      "     2.92231350e+01  2.08781910e+01]\n",
      "   [ 1.00000000e+00  6.70000000e+02  8.08000000e+02 ... -2.10738983e+01\n",
      "     2.92290325e+01  2.08832092e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00  6.88000000e+02  8.88000000e+02 ... -2.14453392e+01\n",
      "     2.93451347e+01  2.09380474e+01]\n",
      "   [ 1.00000000e+00  6.38000000e+02  8.19000000e+02 ... -2.09138985e+01\n",
      "     2.93469925e+01  2.09388638e+01]\n",
      "   [ 1.00000000e+00  5.96000000e+02  7.61000000e+02 ... -2.08924541e+01\n",
      "     2.93483982e+01  2.09394798e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92186794e+01  2.08743992e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92231350e+01  2.08781910e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92290325e+01  2.08832092e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93451347e+01  2.09380474e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93469925e+01  2.09388638e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93483982e+01  2.09394798e+01]]\n",
      "\n",
      "  [[ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92186794e+01  2.08743992e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92231350e+01  2.08781910e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.92290325e+01  2.08832092e+01]\n",
      "   ...\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93451347e+01  2.09380474e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93469925e+01  2.09388638e+01]\n",
      "   [ 1.00000000e+00             nan             nan ...             nan\n",
      "     2.93483982e+01  2.09394798e+01]]]]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to print the first record in the dataset\n",
    "def print_first_record(dataset, dataset_name=\"Dataset\"):\n",
    "    print(f\"--- First Record from {dataset_name} ---\")\n",
    "    for data, label in dataset.take(1):  # Take a single batch from the dataset\n",
    "        print(f\"Data: {data[0]}\")\n",
    "        print(f\"Label: {label[0]}\")\n",
    "\n",
    "# Print the first record for the train dataset\n",
    "print_first_record(train_dataset, \"Train Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base training and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "def train_model(model, num_epochs=50):\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Custom learning rate scheduler\n",
    "    def custom_lr_scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    # Define the path where the best model will be saved\n",
    "    best_model_filepath = f\"{log_dir}/best_model.h5\"\n",
    "    \n",
    "    # ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        best_model_filepath,\n",
    "        monitor='val_loss',  # Change to monitor validation accuracy\n",
    "        save_best_only=True,\n",
    "        mode='min',  # Change to 'max' for accuracy, since we want the highest value\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    callbacks_list = [\n",
    "        tensorboard_callback,\n",
    "        LearningRateScheduler(custom_lr_scheduler, verbose=1),\n",
    "        checkpoint_callback  # Add ModelCheckpoint to callbacks\n",
    "    ]\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=callbacks_list,\n",
    "        # validation_data=val_dataset,\n",
    "    )\n",
    "\n",
    "    # Load the best weights from the saved model\n",
    "    model.load_weights(best_model_filepath)\n",
    "\n",
    "    # Evaluate the best model on the training dataset\n",
    "    train_loss, train_accuracy = model.evaluate(train_dataset, verbose=0)\n",
    "    \n",
    "    # Evaluate the best model on the validation dataset\n",
    "    # val_loss, val_accuracy = model.evaluate(val_dataset, verbose=0)\n",
    "\n",
    "    # Print training and validation accuracy\n",
    "    print(f\"Best model training accuracy: {train_accuracy:.4f}\")\n",
    "    # print(f\"Best model validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Return the model with the best weights\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base 3D CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_3d_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # 3D Convolutional layers\n",
    "    model.add(\n",
    "        layers.Conv3D(64, kernel_size=(3, 3, 3), activation=\"relu\", padding=\"same\")\n",
    "    )\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv3D(32, kernel_size=(3, 3, 3), activation=\"relu\", padding=\"same\")\n",
    "    )\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    # Flatten the output to feed into a Dense layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))  # Dropout layer to reduce overfitting\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))  # Dropout layer to reduce overfitting\n",
    "    \n",
    "    # Output layer for regression (single neuron with linear activation)\n",
    "    model.add(layers.Dense(1))  # No activation function for regression\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# input_shape = (30, 64, 64, 1)  # Adjust this based on your data\n",
    "# model = create_3d_cnn_model(input_shape)\n",
    "\n",
    "# Compile the model for regression\n",
    "# model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Surya Naganathan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv3d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,984</span> \n",
       "\n",
       " max_pooling3d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv3d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">55,328</span> \n",
       "\n",
       " max_pooling3d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,816</span> \n",
       "\n",
       " dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> \n",
       "\n",
       " dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv3d_14 (\u001b[38;5;33mConv3D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m25,984\u001b[0m \n",
       "\n",
       " max_pooling3d_14 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv3d_15 (\u001b[38;5;33mConv3D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          \u001b[38;5;34m55,328\u001b[0m \n",
       "\n",
       " max_pooling3d_15 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)               \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_21 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m4,194,816\u001b[0m \n",
       "\n",
       " dropout_14 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_22 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m131,328\u001b[0m \n",
       "\n",
       " dropout_15 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_23 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                         \u001b[38;5;34m257\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,407,713</span> (16.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,407,713\u001b[0m (16.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,407,713</span> (16.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,407,713\u001b[0m (16.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_3d_cnn = create_3d_cnn_model(timeseries_shape)\n",
    "\n",
    "model_3d_cnn.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "model_3d_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d_cnn = train_model(model=model_3d_cnn, num_epochs=10)\n",
    "\n",
    "test_loss, test_mae = model_3d_cnn.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    # Model definition\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # TimeDistributed wrapper applies a layer to every temporal slice of an input.\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu', padding='same'), input_shape=input_shape))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    \n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu', padding='same')))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same')))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    \n",
    "    # Flatten the spatial dimensions, but keep the temporal dimension\n",
    "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "    \n",
    "    # LSTM layer to process the temporal sequence\n",
    "    model.add(layers.LSTM(256, return_sequences=False))\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 6, 64, 64, 64)     8704      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 6, 32, 32, 64)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 6, 32, 32, 32)     18464     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 6, 16, 16, 32)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 6, 16, 16, 16)     4624      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 6, 8, 8, 16)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 6, 1024)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               1311744   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1607990 (6.13 MB)\n",
      "Trainable params: 1607990 (6.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_cnn_lstm = create_cnn_lstm_model(timeseries_shape, num_classes)\n",
    "\n",
    "model_cnn_lstm.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_cnn_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:04:21.550502: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     27/Unknown - 8s 129ms/step - loss: 1.1302 - accuracy: 0.2874"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:04:29.020901: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4427342384766040563\n",
      "2024-09-19 18:04:29.020911: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15259108753332222775\n",
      "2024-09-19 18:04:29.020915: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16223866063186725963\n",
      "2024-09-19 18:04:29.020918: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13538707906064767957\n",
      "2024-09-19 18:04:29.020923: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15689828834346347568\n",
      "2024-09-19 18:04:29.533696: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7135953658816742753\n",
      "2024-09-19 18:04:29.533711: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17560869367484556627\n",
      "2024-09-19 18:04:29.533715: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10059963876239071109\n",
      "2024-09-19 18:04:29.533720: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8371882402389460458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.34146, saving model to logs/fit/20240919-180420/best_model.h5\n",
      "27/27 [==============================] - 9s 159ms/step - loss: 1.1302 - accuracy: 0.2874 - val_loss: 1.0738 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdifarnaghi/workspace/dailycodes/240125_phenology_kostas/env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/27 [===========================>..] - ETA: 0s - loss: 1.1156 - accuracy: 0.3654\n",
      "Epoch 2: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 1.1113 - accuracy: 0.3640 - val_loss: 1.0564 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0912 - accuracy: 0.3615\n",
      "Epoch 3: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 1.0935 - accuracy: 0.3640 - val_loss: 1.0397 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.0761 - accuracy: 0.3720\n",
      "Epoch 4: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 1.0759 - accuracy: 0.3640 - val_loss: 1.0228 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0566 - accuracy: 0.3654\n",
      "Epoch 5: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 1.0589 - accuracy: 0.3640 - val_loss: 1.0067 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0462 - accuracy: 0.3654\n",
      "Epoch 6: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 1.0422 - accuracy: 0.3640 - val_loss: 0.9912 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0306 - accuracy: 0.3654\n",
      "Epoch 7: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 1.0266 - accuracy: 0.3640 - val_loss: 0.9767 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/200\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.0281 - accuracy: 0.3625\n",
      "Epoch 8: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 1.0117 - accuracy: 0.3640 - val_loss: 0.9625 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9948 - accuracy: 0.3654\n",
      "Epoch 9: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.9971 - accuracy: 0.3640 - val_loss: 0.9489 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9952 - accuracy: 0.3680\n",
      "Epoch 10: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.9826 - accuracy: 0.3640 - val_loss: 0.9350 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009048373904079199.\n",
      "Epoch 11/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9667 - accuracy: 0.3654\n",
      "Epoch 11: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.9689 - accuracy: 0.3640 - val_loss: 0.9230 - val_accuracy: 0.3415 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0008187306812033057.\n",
      "Epoch 12/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9548 - accuracy: 0.3654\n",
      "Epoch 12: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.9570 - accuracy: 0.3640 - val_loss: 0.9122 - val_accuracy: 0.3415 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.000740818097256124.\n",
      "Epoch 13/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9472 - accuracy: 0.3560\n",
      "Epoch 13: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.9465 - accuracy: 0.3640 - val_loss: 0.9028 - val_accuracy: 0.3415 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.000670319888740778.\n",
      "Epoch 14/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9248 - accuracy: 0.3680\n",
      "Epoch 14: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.9370 - accuracy: 0.3640 - val_loss: 0.8947 - val_accuracy: 0.3415 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0006065304623916745.\n",
      "Epoch 15/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9326 - accuracy: 0.3654\n",
      "Epoch 15: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.9290 - accuracy: 0.3640 - val_loss: 0.8872 - val_accuracy: 0.3415 - lr: 6.0653e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0005488114547915757.\n",
      "Epoch 16/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9151 - accuracy: 0.3760\n",
      "Epoch 16: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 45ms/step - loss: 0.9219 - accuracy: 0.3640 - val_loss: 0.8809 - val_accuracy: 0.3415 - lr: 5.4881e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0004965850966982543.\n",
      "Epoch 17/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9133 - accuracy: 0.3654\n",
      "Epoch 17: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.9155 - accuracy: 0.3640 - val_loss: 0.8752 - val_accuracy: 0.3415 - lr: 4.9659e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0004493287415243685.\n",
      "Epoch 18/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9075 - accuracy: 0.3654\n",
      "Epoch 18: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.9097 - accuracy: 0.3640 - val_loss: 0.8701 - val_accuracy: 0.3415 - lr: 4.4933e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0004065694229211658.\n",
      "Epoch 19/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9106 - accuracy: 0.3640\n",
      "Epoch 19: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.9047 - accuracy: 0.3640 - val_loss: 0.8654 - val_accuracy: 0.3415 - lr: 4.0657e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00036787919816561043.\n",
      "Epoch 20/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.9036 - accuracy: 0.3654\n",
      "Epoch 20: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.9002 - accuracy: 0.3640 - val_loss: 0.8615 - val_accuracy: 0.3415 - lr: 3.6788e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0003328708407934755.\n",
      "Epoch 21/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8941 - accuracy: 0.3654\n",
      "Epoch 21: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8963 - accuracy: 0.3640 - val_loss: 0.8580 - val_accuracy: 0.3415 - lr: 3.3287e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00030119396978989244.\n",
      "Epoch 22/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8961 - accuracy: 0.3654\n",
      "Epoch 22: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8927 - accuracy: 0.3640 - val_loss: 0.8547 - val_accuracy: 0.3415 - lr: 3.0119e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.000272531557129696.\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8894 - accuracy: 0.3640\n",
      "Epoch 23: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 46ms/step - loss: 0.8894 - accuracy: 0.3640 - val_loss: 0.8519 - val_accuracy: 0.3415 - lr: 2.7253e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0002465967263560742.\n",
      "Epoch 24/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8899 - accuracy: 0.3654\n",
      "Epoch 24: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8865 - accuracy: 0.3640 - val_loss: 0.8492 - val_accuracy: 0.3415 - lr: 2.4660e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0002231299295090139.\n",
      "Epoch 25/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8821 - accuracy: 0.3615\n",
      "Epoch 25: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8839 - accuracy: 0.3640 - val_loss: 0.8469 - val_accuracy: 0.3415 - lr: 2.2313e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0002018962986767292.\n",
      "Epoch 26/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8823 - accuracy: 0.3560\n",
      "Epoch 26: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8815 - accuracy: 0.3640 - val_loss: 0.8447 - val_accuracy: 0.3415 - lr: 2.0190e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00018268331768922508.\n",
      "Epoch 27/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8737 - accuracy: 0.3640\n",
      "Epoch 27: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8794 - accuracy: 0.3640 - val_loss: 0.8428 - val_accuracy: 0.3415 - lr: 1.8268e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00016529869753867388.\n",
      "Epoch 28/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.9047 - accuracy: 0.3800\n",
      "Epoch 28: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.8776 - accuracy: 0.3640 - val_loss: 0.8412 - val_accuracy: 0.3415 - lr: 1.6530e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00014956844097469002.\n",
      "Epoch 29/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8741 - accuracy: 0.3615\n",
      "Epoch 29: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8759 - accuracy: 0.3640 - val_loss: 0.8397 - val_accuracy: 0.3415 - lr: 1.4957e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0001353351108264178.\n",
      "Epoch 30/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8722 - accuracy: 0.3654\n",
      "Epoch 30: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8743 - accuracy: 0.3640 - val_loss: 0.8382 - val_accuracy: 0.3415 - lr: 1.3534e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00012245627294760197.\n",
      "Epoch 31/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8762 - accuracy: 0.3654\n",
      "Epoch 31: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8729 - accuracy: 0.3640 - val_loss: 0.8370 - val_accuracy: 0.3415 - lr: 1.2246e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00011080301192123443.\n",
      "Epoch 32/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8695 - accuracy: 0.3654\n",
      "Epoch 32: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8716 - accuracy: 0.3640 - val_loss: 0.8358 - val_accuracy: 0.3415 - lr: 1.1080e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00010025870869867504.\n",
      "Epoch 33/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8656 - accuracy: 0.3560\n",
      "Epoch 33: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8705 - accuracy: 0.3640 - val_loss: 0.8348 - val_accuracy: 0.3415 - lr: 1.0026e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 9.071782551473007e-05.\n",
      "Epoch 34/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8650 - accuracy: 0.3520\n",
      "Epoch 34: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8694 - accuracy: 0.3640 - val_loss: 0.8338 - val_accuracy: 0.3415 - lr: 9.0718e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 8.208487997762859e-05.\n",
      "Epoch 35/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8627 - accuracy: 0.3640\n",
      "Epoch 35: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8684 - accuracy: 0.3640 - val_loss: 0.8329 - val_accuracy: 0.3415 - lr: 8.2085e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 7.427347009070218e-05.\n",
      "Epoch 36/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8709 - accuracy: 0.3654\n",
      "Epoch 36: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8676 - accuracy: 0.3640 - val_loss: 0.8321 - val_accuracy: 0.3415 - lr: 7.4273e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 6.720540841342881e-05.\n",
      "Epoch 37/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8510 - accuracy: 0.3520\n",
      "Epoch 37: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8668 - accuracy: 0.3640 - val_loss: 0.8315 - val_accuracy: 0.3415 - lr: 6.7205e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 6.080996536184102e-05.\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8661 - accuracy: 0.3640\n",
      "Epoch 38: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8661 - accuracy: 0.3640 - val_loss: 0.8308 - val_accuracy: 0.3415 - lr: 6.0810e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 5.502313069882803e-05.\n",
      "Epoch 39/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8688 - accuracy: 0.3654\n",
      "Epoch 39: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8655 - accuracy: 0.3640 - val_loss: 0.8302 - val_accuracy: 0.3415 - lr: 5.5023e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 4.978698416380212e-05.\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8649 - accuracy: 0.3640\n",
      "Epoch 40: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8649 - accuracy: 0.3640 - val_loss: 0.8298 - val_accuracy: 0.3415 - lr: 4.9787e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 4.5049124310025945e-05.\n",
      "Epoch 41/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8623 - accuracy: 0.3654\n",
      "Epoch 41: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8644 - accuracy: 0.3640 - val_loss: 0.8293 - val_accuracy: 0.3415 - lr: 4.5049e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 4.076213008374907e-05.\n",
      "Epoch 42/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8535 - accuracy: 0.3560\n",
      "Epoch 42: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.8640 - accuracy: 0.3640 - val_loss: 0.8289 - val_accuracy: 0.3415 - lr: 4.0762e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 3.688309880089946e-05.\n",
      "Epoch 43/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8668 - accuracy: 0.3654\n",
      "Epoch 43: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8635 - accuracy: 0.3640 - val_loss: 0.8285 - val_accuracy: 0.3415 - lr: 3.6883e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 3.337320595164783e-05.\n",
      "Epoch 44/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8665 - accuracy: 0.3654\n",
      "Epoch 44: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8631 - accuracy: 0.3640 - val_loss: 0.8282 - val_accuracy: 0.3415 - lr: 3.3373e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 3.019732321263291e-05.\n",
      "Epoch 45/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8607 - accuracy: 0.3654\n",
      "Epoch 45: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8628 - accuracy: 0.3640 - val_loss: 0.8279 - val_accuracy: 0.3415 - lr: 3.0197e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.7323667382006533e-05.\n",
      "Epoch 46/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8620 - accuracy: 0.3680\n",
      "Epoch 46: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8625 - accuracy: 0.3640 - val_loss: 0.8276 - val_accuracy: 0.3415 - lr: 2.7324e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 2.4723474780330434e-05.\n",
      "Epoch 47/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8605 - accuracy: 0.3615\n",
      "Epoch 47: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8622 - accuracy: 0.3640 - val_loss: 0.8273 - val_accuracy: 0.3415 - lr: 2.4723e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 2.237072476418689e-05.\n",
      "Epoch 48/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8598 - accuracy: 0.3654\n",
      "Epoch 48: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8620 - accuracy: 0.3640 - val_loss: 0.8271 - val_accuracy: 0.3415 - lr: 2.2371e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 2.024186687776819e-05.\n",
      "Epoch 49/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8600 - accuracy: 0.3615\n",
      "Epoch 49: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8617 - accuracy: 0.3640 - val_loss: 0.8269 - val_accuracy: 0.3415 - lr: 2.0242e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1.8315597117180005e-05.\n",
      "Epoch 50/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8598 - accuracy: 0.3615\n",
      "Epoch 50: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.8615 - accuracy: 0.3640 - val_loss: 0.8267 - val_accuracy: 0.3415 - lr: 1.8316e-05\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 1.6572636013734154e-05.\n",
      "Epoch 51/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8596 - accuracy: 0.3615\n",
      "Epoch 51: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8613 - accuracy: 0.3640 - val_loss: 0.8265 - val_accuracy: 0.3415 - lr: 1.6573e-05\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 1.4995540368545335e-05.\n",
      "Epoch 52/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8645 - accuracy: 0.3654\n",
      "Epoch 52: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8611 - accuracy: 0.3640 - val_loss: 0.8264 - val_accuracy: 0.3415 - lr: 1.4996e-05\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 1.3568524991569575e-05.\n",
      "Epoch 53/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8553 - accuracy: 0.3640\n",
      "Epoch 53: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8610 - accuracy: 0.3640 - val_loss: 0.8262 - val_accuracy: 0.3415 - lr: 1.3569e-05\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 1.2277308087504935e-05.\n",
      "Epoch 54/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8592 - accuracy: 0.3615\n",
      "Epoch 54: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8609 - accuracy: 0.3640 - val_loss: 0.8261 - val_accuracy: 0.3415 - lr: 1.2277e-05\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 1.1108967555628624e-05.\n",
      "Epoch 55/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8586 - accuracy: 0.3654\n",
      "Epoch 55: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8607 - accuracy: 0.3640 - val_loss: 0.8260 - val_accuracy: 0.3415 - lr: 1.1109e-05\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 1.0051809113065246e-05.\n",
      "Epoch 56/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8589 - accuracy: 0.3615\n",
      "Epoch 56: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8606 - accuracy: 0.3640 - val_loss: 0.8259 - val_accuracy: 0.3415 - lr: 1.0052e-05\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 9.095252607949078e-06.\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8605 - accuracy: 0.3640\n",
      "Epoch 57: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8605 - accuracy: 0.3640 - val_loss: 0.8258 - val_accuracy: 0.3415 - lr: 9.0953e-06\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 8.229724699049257e-06.\n",
      "Epoch 58/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8665 - accuracy: 0.3600\n",
      "Epoch 58: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.8604 - accuracy: 0.3640 - val_loss: 0.8257 - val_accuracy: 0.3415 - lr: 8.2297e-06\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 7.446562449331395e-06.\n",
      "Epoch 59/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8636 - accuracy: 0.3654\n",
      "Epoch 59: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8603 - accuracy: 0.3640 - val_loss: 0.8256 - val_accuracy: 0.3415 - lr: 7.4466e-06\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 6.737927833455615e-06.\n",
      "Epoch 60/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8636 - accuracy: 0.3654\n",
      "Epoch 60: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8602 - accuracy: 0.3640 - val_loss: 0.8256 - val_accuracy: 0.3415 - lr: 6.7379e-06\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 6.096729066484841e-06.\n",
      "Epoch 61/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8585 - accuracy: 0.3615\n",
      "Epoch 61: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8602 - accuracy: 0.3640 - val_loss: 0.8255 - val_accuracy: 0.3415 - lr: 6.0967e-06\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 5.516548299056012e-06.\n",
      "Epoch 62/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8580 - accuracy: 0.3654\n",
      "Epoch 62: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8601 - accuracy: 0.3640 - val_loss: 0.8254 - val_accuracy: 0.3415 - lr: 5.5165e-06\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 4.99157886224566e-06.\n",
      "Epoch 63/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8634 - accuracy: 0.3654\n",
      "Epoch 63: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8601 - accuracy: 0.3640 - val_loss: 0.8254 - val_accuracy: 0.3415 - lr: 4.9916e-06\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 4.5165670599089935e-06.\n",
      "Epoch 64/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8633 - accuracy: 0.3654\n",
      "Epoch 64: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8600 - accuracy: 0.3640 - val_loss: 0.8253 - val_accuracy: 0.3415 - lr: 4.5166e-06\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 4.086758508492494e-06.\n",
      "Epoch 65/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8552 - accuracy: 0.3560\n",
      "Epoch 65: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8600 - accuracy: 0.3640 - val_loss: 0.8253 - val_accuracy: 0.3415 - lr: 4.0868e-06\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 3.6978517528041266e-06.\n",
      "Epoch 66/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8604 - accuracy: 0.3600\n",
      "Epoch 66: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8599 - accuracy: 0.3640 - val_loss: 0.8253 - val_accuracy: 0.3415 - lr: 3.6979e-06\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 3.3459543828939786e-06.\n",
      "Epoch 67/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8632 - accuracy: 0.3654\n",
      "Epoch 67: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8599 - accuracy: 0.3640 - val_loss: 0.8252 - val_accuracy: 0.3415 - lr: 3.3460e-06\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 3.02754460790311e-06.\n",
      "Epoch 68/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8577 - accuracy: 0.3654\n",
      "Epoch 68: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8598 - accuracy: 0.3640 - val_loss: 0.8252 - val_accuracy: 0.3415 - lr: 3.0275e-06\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 2.739435558396508e-06.\n",
      "Epoch 69/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8655 - accuracy: 0.3640\n",
      "Epoch 69: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8598 - accuracy: 0.3640 - val_loss: 0.8252 - val_accuracy: 0.3415 - lr: 2.7394e-06\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 2.478743681422202e-06.\n",
      "Epoch 70/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8631 - accuracy: 0.3654\n",
      "Epoch 70: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8598 - accuracy: 0.3640 - val_loss: 0.8251 - val_accuracy: 0.3415 - lr: 2.4787e-06\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 2.2428598640544806e-06.\n",
      "Epoch 71/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8581 - accuracy: 0.3615\n",
      "Epoch 71: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 45ms/step - loss: 0.8598 - accuracy: 0.3640 - val_loss: 0.8251 - val_accuracy: 0.3415 - lr: 2.2429e-06\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 2.0294235127948923e-06.\n",
      "Epoch 72/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8630 - accuracy: 0.3654\n",
      "Epoch 72: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8597 - accuracy: 0.3640 - val_loss: 0.8251 - val_accuracy: 0.3415 - lr: 2.0294e-06\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 1.8362982245889725e-06.\n",
      "Epoch 73/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8576 - accuracy: 0.3654\n",
      "Epoch 73: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8597 - accuracy: 0.3640 - val_loss: 0.8251 - val_accuracy: 0.3415 - lr: 1.8363e-06\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 1.661551209508616e-06.\n",
      "Epoch 74/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8575 - accuracy: 0.3654\n",
      "Epoch 74: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8597 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.6616e-06\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 1.5034336229291512e-06.\n",
      "Epoch 75/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8575 - accuracy: 0.3654\n",
      "Epoch 75: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8597 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.5034e-06\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 1.360362944069493e-06.\n",
      "Epoch 76/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8580 - accuracy: 0.3615\n",
      "Epoch 76: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8597 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.3604e-06\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 1.2309071735217003e-06.\n",
      "Epoch 77/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8700 - accuracy: 0.3720\n",
      "Epoch 77: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.2309e-06\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 1.113770849769935e-06.\n",
      "Epoch 78/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8591 - accuracy: 0.3680\n",
      "Epoch 78: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.1138e-06\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 1.0077815204567742e-06.\n",
      "Epoch 79/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8629 - accuracy: 0.3654\n",
      "Epoch 79: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 45ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 1.0078e-06\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 9.118783736994374e-07.\n",
      "Epoch 80/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8629 - accuracy: 0.3654\n",
      "Epoch 80: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 9.1188e-07\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 8.251016083704599e-07.\n",
      "Epoch 81/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8579 - accuracy: 0.3615\n",
      "Epoch 81: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 8.2510e-07\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 7.465827707164863e-07.\n",
      "Epoch 82/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 82: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8250 - val_accuracy: 0.3415 - lr: 7.4658e-07\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 6.755360004717659e-07.\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8596 - accuracy: 0.3640\n",
      "Epoch 83: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 45ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.7554e-07\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 6.112502433097688e-07.\n",
      "Epoch 84/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8629 - accuracy: 0.3654\n",
      "Epoch 84: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 42ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1125e-07\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 5.530820885724097e-07.\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8596 - accuracy: 0.3640\n",
      "Epoch 85: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.5308e-07\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 5.00449345963716e-07.\n",
      "Epoch 86/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 86: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.0045e-07\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 4.528252759428142e-07.\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8596 - accuracy: 0.3640\n",
      "Epoch 87: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.5283e-07\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 4.0973321802084683e-07.\n",
      "Epoch 88/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 88: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8596 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.0973e-07\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 3.7074192960062646e-07.\n",
      "Epoch 89/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 89: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.7074e-07\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 3.354611521899642e-07.\n",
      "Epoch 90/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8552 - accuracy: 0.3520\n",
      "Epoch 90: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.3546e-07\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 3.035377744708967e-07.\n",
      "Epoch 91/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 91: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.0354e-07\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 2.746523080077168e-07.\n",
      "Epoch 92/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 92: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.7465e-07\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 2.4851567559380783e-07.\n",
      "Epoch 93/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8652 - accuracy: 0.3640\n",
      "Epoch 93: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 45ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.4852e-07\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 2.2486626960471767e-07.\n",
      "Epoch 94/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8590 - accuracy: 0.3680\n",
      "Epoch 94: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.2487e-07\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 2.0346740825516463e-07.\n",
      "Epoch 95/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 95: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.0347e-07\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 1.8410491975373589e-07.\n",
      "Epoch 96/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8491 - accuracy: 0.3560\n",
      "Epoch 96: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.8410e-07\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 1.6658501067468023e-07.\n",
      "Epoch 97/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 97: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.6659e-07\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 1.507323474925215e-07.\n",
      "Epoch 98/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 98: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.5073e-07\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 1.363882660143645e-07.\n",
      "Epoch 99/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8486 - accuracy: 0.3600\n",
      "Epoch 99: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.3639e-07\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 1.2340919397502148e-07.\n",
      "Epoch 100/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 100: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.2341e-07\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 1.116652512678229e-07.\n",
      "Epoch 101/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 101: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.1167e-07\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 1.0103889280799194e-07.\n",
      "Epoch 102/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 102: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.0104e-07\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 9.142376455884005e-08.\n",
      "Epoch 103/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 103: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 9.1424e-08\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 8.272363771766322e-08.\n",
      "Epoch 104/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 104: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 8.2724e-08\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 7.485143527219407e-08.\n",
      "Epoch 105/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8590 - accuracy: 0.3680\n",
      "Epoch 105: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 7.4851e-08\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 6.772837934931886e-08.\n",
      "Epoch 106/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 106: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.7728e-08\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 6.128316698550407e-08.\n",
      "Epoch 107/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8812 - accuracy: 0.3720\n",
      "Epoch 107: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1283e-08\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 5.5451298663911075e-08.\n",
      "Epoch 108/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 108: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.5451e-08\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 5.0174406851510867e-08.\n",
      "Epoch 109/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 109: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.0174e-08\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 4.5399676906754394e-08.\n",
      "Epoch 110/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8661 - accuracy: 0.3560\n",
      "Epoch 110: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.5400e-08\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 4.107932483066179e-08.\n",
      "Epoch 111/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 111: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.1079e-08\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 3.7170106992334695e-08.\n",
      "Epoch 112/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8661 - accuracy: 0.3560\n",
      "Epoch 112: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.7170e-08\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 3.363290090874216e-08.\n",
      "Epoch 113/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 113: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.3633e-08\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 3.043230734078861e-08.\n",
      "Epoch 114/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 114: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.0432e-08\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 2.753628969287547e-08.\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 115: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.7536e-08\n",
      "\n",
      "Epoch 116: LearningRateScheduler setting learning rate to 2.4915863150454243e-08.\n",
      "Epoch 116/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8552 - accuracy: 0.3520\n",
      "Epoch 116: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.4916e-08\n",
      "\n",
      "Epoch 117: LearningRateScheduler setting learning rate to 2.254480335750486e-08.\n",
      "Epoch 117/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 117: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.2545e-08\n",
      "\n",
      "Epoch 118: LearningRateScheduler setting learning rate to 2.0399379963009778e-08.\n",
      "Epoch 118/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 118: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.0399e-08\n",
      "\n",
      "Epoch 119: LearningRateScheduler setting learning rate to 1.8458122141851163e-08.\n",
      "Epoch 119/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 119: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.8458e-08\n",
      "\n",
      "Epoch 120: LearningRateScheduler setting learning rate to 1.6701598326562817e-08.\n",
      "Epoch 120/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8586 - accuracy: 0.3720\n",
      "Epoch 120: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 44ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.6702e-08\n",
      "\n",
      "Epoch 121: LearningRateScheduler setting learning rate to 1.5112229689862033e-08.\n",
      "Epoch 121/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8656 - accuracy: 0.3600\n",
      "Epoch 121: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.5112e-08\n",
      "\n",
      "Epoch 122: LearningRateScheduler setting learning rate to 1.3674109844430404e-08.\n",
      "Epoch 122/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8534 - accuracy: 0.3680\n",
      "Epoch 122: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.3674e-08\n",
      "\n",
      "Epoch 123: LearningRateScheduler setting learning rate to 1.2372845858976689e-08.\n",
      "Epoch 123/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 123: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.2373e-08\n",
      "\n",
      "Epoch 124: LearningRateScheduler setting learning rate to 1.1195413485154404e-08.\n",
      "Epoch 124/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8477 - accuracy: 0.3680\n",
      "Epoch 124: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.1195e-08\n",
      "\n",
      "Epoch 125: LearningRateScheduler setting learning rate to 1.0130028371690969e-08.\n",
      "Epoch 125/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 125: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.0130e-08\n",
      "\n",
      "Epoch 126: LearningRateScheduler setting learning rate to 9.166027936657883e-09.\n",
      "Epoch 126/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8538 - accuracy: 0.3640\n",
      "Epoch 126: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 9.1660e-09\n",
      "\n",
      "Epoch 127: LearningRateScheduler setting learning rate to 8.293764786060365e-09.\n",
      "Epoch 127/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 127: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 8.2938e-09\n",
      "\n",
      "Epoch 128: LearningRateScheduler setting learning rate to 7.50450812603276e-09.\n",
      "Epoch 128/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 128: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 7.5045e-09\n",
      "\n",
      "Epoch 129: LearningRateScheduler setting learning rate to 6.790359385888678e-09.\n",
      "Epoch 129/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8590 - accuracy: 0.3680\n",
      "Epoch 129: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.7904e-09\n",
      "\n",
      "Epoch 130: LearningRateScheduler setting learning rate to 6.14417094979558e-09.\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 130: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1442e-09\n",
      "\n",
      "Epoch 131: LearningRateScheduler setting learning rate to 5.55947554659042e-09.\n",
      "Epoch 131/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 131: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.5595e-09\n",
      "\n",
      "Epoch 132: LearningRateScheduler setting learning rate to 5.030421412755004e-09.\n",
      "Epoch 132/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8599 - accuracy: 0.3600\n",
      "Epoch 132: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 46ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.0304e-09\n",
      "\n",
      "Epoch 133: LearningRateScheduler setting learning rate to 4.55171322855108e-09.\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 133: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.5517e-09\n",
      "\n",
      "Epoch 134: LearningRateScheduler setting learning rate to 4.118560159582785e-09.\n",
      "Epoch 134/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 134: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.1186e-09\n",
      "\n",
      "Epoch 135: LearningRateScheduler setting learning rate to 3.726627006983563e-09.\n",
      "Epoch 135/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8760 - accuracy: 0.3680\n",
      "Epoch 135: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.7266e-09\n",
      "\n",
      "Epoch 136: LearningRateScheduler setting learning rate to 3.371991352807413e-09.\n",
      "Epoch 136/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 136: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.3720e-09\n",
      "\n",
      "Epoch 137: LearningRateScheduler setting learning rate to 3.051103814044609e-09.\n",
      "Epoch 137/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 137: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.0511e-09\n",
      "\n",
      "Epoch 138: LearningRateScheduler setting learning rate to 2.760752737529515e-09.\n",
      "Epoch 138/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 138: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.7608e-09\n",
      "\n",
      "Epoch 139: LearningRateScheduler setting learning rate to 2.498032225517477e-09.\n",
      "Epoch 139/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 139: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.4980e-09\n",
      "\n",
      "Epoch 140: LearningRateScheduler setting learning rate to 2.260312825796973e-09.\n",
      "Epoch 140/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8434 - accuracy: 0.3560\n",
      "Epoch 140: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.2603e-09\n",
      "\n",
      "Epoch 141: LearningRateScheduler setting learning rate to 2.045215552470836e-09.\n",
      "Epoch 141/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 141: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.0452e-09\n",
      "\n",
      "Epoch 142: LearningRateScheduler setting learning rate to 1.850587461049713e-09.\n",
      "Epoch 142/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 142: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.8506e-09\n",
      "\n",
      "Epoch 143: LearningRateScheduler setting learning rate to 1.6744806652368993e-09.\n",
      "Epoch 143/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8439 - accuracy: 0.3520\n",
      "Epoch 143: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.6745e-09\n",
      "\n",
      "Epoch 144: LearningRateScheduler setting learning rate to 1.5151326859808023e-09.\n",
      "Epoch 144/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 144: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.5151e-09\n",
      "\n",
      "Epoch 145: LearningRateScheduler setting learning rate to 1.370948687906548e-09.\n",
      "Epoch 145/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8486 - accuracy: 0.3600\n",
      "Epoch 145: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.3709e-09\n",
      "\n",
      "Epoch 146: LearningRateScheduler setting learning rate to 1.2404856031267286e-09.\n",
      "Epoch 146/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 146: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.2405e-09\n",
      "\n",
      "Epoch 147: LearningRateScheduler setting learning rate to 1.1224376983420825e-09.\n",
      "Epoch 147/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 147: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.1224e-09\n",
      "\n",
      "Epoch 148: LearningRateScheduler setting learning rate to 1.015623585232106e-09.\n",
      "Epoch 148/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8604 - accuracy: 0.3560\n",
      "Epoch 148: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.0156e-09\n",
      "\n",
      "Epoch 149: LearningRateScheduler setting learning rate to 9.189741745352364e-10.\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 149: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 9.1897e-10\n",
      "\n",
      "Epoch 150: LearningRateScheduler setting learning rate to 8.31522184441269e-10.\n",
      "Epoch 150/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 150: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 8.3152e-10\n",
      "\n",
      "Epoch 151: LearningRateScheduler setting learning rate to 7.523923706287405e-10.\n",
      "Epoch 151/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8543 - accuracy: 0.3600\n",
      "Epoch 151: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 7.5239e-10\n",
      "\n",
      "Epoch 152: LearningRateScheduler setting learning rate to 6.807927555030346e-10.\n",
      "Epoch 152/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8652 - accuracy: 0.3640\n",
      "Epoch 152: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.8079e-10\n",
      "\n",
      "Epoch 153: LearningRateScheduler setting learning rate to 6.16006745612907e-10.\n",
      "Epoch 153/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 153: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1601e-10\n",
      "\n",
      "Epoch 154: LearningRateScheduler setting learning rate to 5.573859152008254e-10.\n",
      "Epoch 154/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8524 - accuracy: 0.3760\n",
      "Epoch 154: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.5739e-10\n",
      "\n",
      "Epoch 155: LearningRateScheduler setting learning rate to 5.043436224205777e-10.\n",
      "Epoch 155/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 155: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.0434e-10\n",
      "\n",
      "Epoch 156: LearningRateScheduler setting learning rate to 4.5634895862178837e-10.\n",
      "Epoch 156/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8708 - accuracy: 0.3640\n",
      "Epoch 156: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.5635e-10\n",
      "\n",
      "Epoch 157: LearningRateScheduler setting learning rate to 4.129215858128532e-10.\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 157: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.1292e-10\n",
      "\n",
      "Epoch 158: LearningRateScheduler setting learning rate to 3.736268794352071e-10.\n",
      "Epoch 158/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8586 - accuracy: 0.3720\n",
      "Epoch 158: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.7363e-10\n",
      "\n",
      "Epoch 159: LearningRateScheduler setting learning rate to 3.3807157073795224e-10.\n",
      "Epoch 159/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 159: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.3807e-10\n",
      "\n",
      "Epoch 160: LearningRateScheduler setting learning rate to 3.05899777730545e-10.\n",
      "Epoch 160/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8477 - accuracy: 0.3680\n",
      "Epoch 160: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.0590e-10\n",
      "\n",
      "Epoch 161: LearningRateScheduler setting learning rate to 2.767895634914197e-10.\n",
      "Epoch 161/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 161: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.7679e-10\n",
      "\n",
      "Epoch 162: LearningRateScheduler setting learning rate to 2.504495499877635e-10.\n",
      "Epoch 162/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 162: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.5045e-10\n",
      "\n",
      "Epoch 163: LearningRateScheduler setting learning rate to 2.2661611476237908e-10.\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 163: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 43ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.2662e-10\n",
      "\n",
      "Epoch 164: LearningRateScheduler setting learning rate to 2.050507263984258e-10.\n",
      "Epoch 164/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 164: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.0505e-10\n",
      "\n",
      "Epoch 165: LearningRateScheduler setting learning rate to 1.8553755753991652e-10.\n",
      "Epoch 165/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8604 - accuracy: 0.3560\n",
      "Epoch 165: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.8554e-10\n",
      "\n",
      "Epoch 166: LearningRateScheduler setting learning rate to 1.6788131995681965e-10.\n",
      "Epoch 166/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 166: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.6788e-10\n",
      "\n",
      "Epoch 167: LearningRateScheduler setting learning rate to 1.519052938991905e-10.\n",
      "Epoch 167/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8599 - accuracy: 0.3600\n",
      "Epoch 167: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.5191e-10\n",
      "\n",
      "Epoch 168: LearningRateScheduler setting learning rate to 1.3744959337369522e-10.\n",
      "Epoch 168/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8694 - accuracy: 0.3760\n",
      "Epoch 168: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.3745e-10\n",
      "\n",
      "Epoch 169: LearningRateScheduler setting learning rate to 1.2436952856464956e-10.\n",
      "Epoch 169/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 169: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.2437e-10\n",
      "\n",
      "Epoch 170: LearningRateScheduler setting learning rate to 1.1253419723855629e-10.\n",
      "Epoch 170/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 170: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.1253e-10\n",
      "\n",
      "Epoch 171: LearningRateScheduler setting learning rate to 1.0182514553758182e-10.\n",
      "Epoch 171/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 171: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.0183e-10\n",
      "\n",
      "Epoch 172: LearningRateScheduler setting learning rate to 9.21351953064864e-11.\n",
      "Epoch 172/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 172: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 9.2135e-11\n",
      "\n",
      "Epoch 173: LearningRateScheduler setting learning rate to 8.336736856406901e-11.\n",
      "Epoch 173/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 173: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 8.3367e-11\n",
      "\n",
      "Epoch 174: LearningRateScheduler setting learning rate to 7.543390911912695e-11.\n",
      "Epoch 174/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 174: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 7.5434e-11\n",
      "\n",
      "Epoch 175: LearningRateScheduler setting learning rate to 6.825542214761171e-11.\n",
      "Epoch 175/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8604 - accuracy: 0.3560\n",
      "Epoch 175: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.8255e-11\n",
      "\n",
      "Epoch 176: LearningRateScheduler setting learning rate to 6.176005540314833e-11.\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 176: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1760e-11\n",
      "\n",
      "Epoch 177: LearningRateScheduler setting learning rate to 5.5882805327645e-11.\n",
      "Epoch 177/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 177: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.5883e-11\n",
      "\n",
      "Epoch 178: LearningRateScheduler setting learning rate to 5.056485091747831e-11.\n",
      "Epoch 178/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 178: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.0565e-11\n",
      "\n",
      "Epoch 179: LearningRateScheduler setting learning rate to 4.575296738695833e-11.\n",
      "Epoch 179/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 179: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.5753e-11\n",
      "\n",
      "Epoch 180: LearningRateScheduler setting learning rate to 4.1398995342945e-11.\n",
      "Epoch 180/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 180: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 4.1399e-11\n",
      "\n",
      "Epoch 181: LearningRateScheduler setting learning rate to 3.7459358531721776e-11.\n",
      "Epoch 181/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 181: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.7459e-11\n",
      "\n",
      "Epoch 182: LearningRateScheduler setting learning rate to 3.389462668867971e-11.\n",
      "Epoch 182/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 182: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 2s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.3895e-11\n",
      "\n",
      "Epoch 183: LearningRateScheduler setting learning rate to 3.0669123490811856e-11.\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 183: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 3.0669e-11\n",
      "\n",
      "Epoch 184: LearningRateScheduler setting learning rate to 2.7750569203677244e-11.\n",
      "Epoch 184/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 184: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.7751e-11\n",
      "\n",
      "Epoch 185: LearningRateScheduler setting learning rate to 2.51097528186639e-11.\n",
      "Epoch 185/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 185: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.5110e-11\n",
      "\n",
      "Epoch 186: LearningRateScheduler setting learning rate to 2.2720242354168363e-11.\n",
      "Epoch 186/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 186: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.2720e-11\n",
      "\n",
      "Epoch 187: LearningRateScheduler setting learning rate to 2.0558124647074294e-11.\n",
      "Epoch 187/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 187: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 2.0558e-11\n",
      "\n",
      "Epoch 188: LearningRateScheduler setting learning rate to 1.8601759022018882e-11.\n",
      "Epoch 188/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 188: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.8602e-11\n",
      "\n",
      "Epoch 189: LearningRateScheduler setting learning rate to 1.6831567389852253e-11.\n",
      "Epoch 189/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 189: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.6832e-11\n",
      "\n",
      "Epoch 190: LearningRateScheduler setting learning rate to 1.522983128499078e-11.\n",
      "Epoch 190/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 190: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.5230e-11\n",
      "\n",
      "Epoch 191: LearningRateScheduler setting learning rate to 1.3780520127792961e-11.\n",
      "Epoch 191/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8486 - accuracy: 0.3600\n",
      "Epoch 191: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.3781e-11\n",
      "\n",
      "Epoch 192: LearningRateScheduler setting learning rate to 1.2469129895276154e-11.\n",
      "Epoch 192/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8578 - accuracy: 0.3615\n",
      "Epoch 192: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.2469e-11\n",
      "\n",
      "Epoch 193: LearningRateScheduler setting learning rate to 1.128253480225938e-11.\n",
      "Epoch 193/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 193: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.1283e-11\n",
      "\n",
      "Epoch 194: LearningRateScheduler setting learning rate to 1.0208858931826104e-11.\n",
      "Epoch 194/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8642 - accuracy: 0.3720\n",
      "Epoch 194: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 1.0209e-11\n",
      "\n",
      "Epoch 195: LearningRateScheduler setting learning rate to 9.237357406766122e-12.\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 195: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 9.2374e-12\n",
      "\n",
      "Epoch 196: LearningRateScheduler setting learning rate to 8.358306234634849e-12.\n",
      "Epoch 196/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 196: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 8.3583e-12\n",
      "\n",
      "Epoch 197: LearningRateScheduler setting learning rate to 7.562907765323867e-12.\n",
      "Epoch 197/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8628 - accuracy: 0.3654\n",
      "Epoch 197: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 7.5629e-12\n",
      "\n",
      "Epoch 198: LearningRateScheduler setting learning rate to 6.843201786482789e-12.\n",
      "Epoch 198/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8590 - accuracy: 0.3680\n",
      "Epoch 198: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.8432e-12\n",
      "\n",
      "Epoch 199: LearningRateScheduler setting learning rate to 6.191984858877619e-12.\n",
      "Epoch 199/200\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.8595 - accuracy: 0.3640\n",
      "Epoch 199: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 6.1920e-12\n",
      "\n",
      "Epoch 200: LearningRateScheduler setting learning rate to 5.6027391927282455e-12.\n",
      "Epoch 200/200\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.3654\n",
      "Epoch 200: val_accuracy did not improve from 0.34146\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.8595 - accuracy: 0.3640 - val_loss: 0.8249 - val_accuracy: 0.3415 - lr: 5.6027e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:09:25.313950: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 11335459269281643048\n",
      "2024-09-19 18:09:25.313965: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8371882402389460458\n",
      "2024-09-19 18:09:25.313978: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7135953658816742753\n",
      "2024-09-19 18:09:25.313988: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17560869367484556627\n",
      "2024-09-19 18:09:25.313991: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10059963876239071109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model training accuracy: 0.3640\n",
      "Best model validation accuracy: 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:09:25.536502: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7135953658816742753\n",
      "2024-09-19 18:09:25.536514: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17560869367484556627\n",
      "2024-09-19 18:09:25.536518: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10059963876239071109\n",
      "2024-09-19 18:09:25.536521: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8371882402389460458\n"
     ]
    }
   ],
   "source": [
    "model_cnn_lstm = train_model(model_cnn_lstm, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1523 - accuracy: 0.3457\n",
      "Test Loss: 1.1523404121398926\n",
      "Test Accuracy: 0.34567901492118835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:09:25.969977: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7135953658816742753\n",
      "2024-09-19 18:09:25.969998: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17560869367484556627\n",
      "2024-09-19 18:09:25.970001: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10059963876239071109\n",
      "2024-09-19 18:09:25.970005: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8371882402389460458\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model_cnn_lstm.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 24ms/step - loss: 1.1200 - accuracy: 0.3640\n",
      "Train Loss: 1.119980812072754\n",
      "Train Accuracy: 0.3639846742153168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 18:09:27.005688: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7135953658816742753\n",
      "2024-09-19 18:09:27.005703: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17560869367484556627\n",
      "2024-09-19 18:09:27.005707: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10059963876239071109\n",
      "2024-09-19 18:09:27.005711: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8371882402389460458\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "train_loss, train_accuracy = model_cnn_lstm.evaluate(train_dataset)\n",
    "\n",
    "print(f\"Train Loss: {train_loss}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
